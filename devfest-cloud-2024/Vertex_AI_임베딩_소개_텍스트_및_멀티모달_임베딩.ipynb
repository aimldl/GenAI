{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijGzTHJJUCPY"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDsTUvKjwHBW"
      },
      "source": [
        "# Vertex AI 임베딩 소개 - 텍스트 및 멀티모달 임베딩\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/qa-ops/intro_Vertex_AI_embeddings.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fqa-ops%2Fintro_Vertex_AI_embeddings.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/qa-ops/intro_Vertex_AI_embeddings.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/qa-ops/intro_Vertex_AI_embeddings.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>    \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uoRmYQsKBgl"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Lavi Nigam](https://github.com/lavinigam-gcp) , [Kaz Sato]()  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK1Q5ZYdVL4Y"
      },
      "source": [
        "## 개요\n",
        "\n",
        "\n",
        "이 노트북에서는 텍스트와 멀티모달(이미지 및 비디오) 모두를 위한 Vertex AI Embeddings API를 살펴봅니다. 바로 시작하기 전에 임베딩이 무엇인지 이해해 볼까요?\n",
        "\n",
        "\n",
        "\n",
        "**임베딩: 콘텐츠를 숫자로 변환**\n",
        "\n",
        "\n",
        "친구의 성격을 다른 사람에게 설명하려고 한다고 상상해 보세요. \"친절하다\", \"에너지가 넘친다\", \"사려 깊다\"와 같은 단어를 사용할 수 있습니다. 하지만 이러한 각 특성에 일련의 숫자를 할당하여 친구를 위한 고유한 코드를 만들 수 있다면 멋지지 않을까요?\n",
        "\n",
        "\n",
        "임베딩은 다양한 유형의 콘텐츠에 대해 유사한 작업을 수행합니다.\n",
        "\n",
        "\n",
        "* **텍스트의 경우:** 각 단어, 문장 또는 전체 문서도 숫자 목록(벡터)으로 변환됩니다. 이 숫자는 단어 간의 의미와 관계를 포착합니다. 예를 들어, \"cat\"이라는 단어는 [0.25, -0.18, 0.93...]으로 표시될 수 있고 \"kitten\"은 [0.30, -0.16, 0.88...]로 표시될 수 있습니다. 이러한 벡터의 근접성은 의미론적 연결을 나타냅니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **이미지의 경우:** 픽셀을 대신하여 이미지의 시각적 특징을 나타내는 벡터가 됩니다. 햇볕이 잘 드는 해변의 사진은 [0.85, 0.42, -0.05...]로 변환될 수 있는 반면, 눈 덮인 산은 [-0.32, 0.78, 0.12...]로 변환될 수 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **동영상의 경우:** 각 프레임 또는 전체 동영상 시퀀스는 시각적 콘텐츠, 움직임 및 잠재적으로 오디오 정보까지 캡슐화하는 수치 표현을 얻습니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "또 다른 예를 들어 보겠습니다. 책, 사진, 영화 DVD 등 일치하지 않는 항목이 들어 있는 거대한 상자가 있다고 상상해 보십시오. 각 항목은 독특하고 복잡하지만, 이를 이해하기 쉬운 방식으로 구성하고 싶습니다. 이런 역할을 수행해주는 것이 임베딩입니다. 텍스트, 이미지, 비디오와 같은 데이터에만 적용되지만요.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "기본적으로 임베딩은 데이터를 단순화된 방식으로 나타내는 비밀 코드와 같습니다. 지도의 숫자 좌표로 생각해보세요. 유사한 항목(예: 동일한 주제에 대한 책, 동일한 장소의 사진)은 이 지도에서 서로 가까이 있는 반면, 매우 다른 항목은 멀리 떨어져 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjB8ZjusCQn6"
      },
      "source": [
        "**임베딩이 필요한 이유는 무엇입니까?**\n",
        "\n",
        "\n",
        "컴퓨터는 숫자를 처리하는 데는 탁월하지만 텍스트, 이미지 또는 비디오를 이해하는 데는 어려움을 겪습니다. 임베딩은 변환기처럼 작동하여 복잡한 내용을 컴퓨터가 쉽게 사용할 수 있는 형식으로 변환합니다. 이는 다음과 같은 작업에 대한 가능성의 세계를 열어줍니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **유사점 찾기:** 이러한 벡터의 숫자를 비교하면 콘텐츠가 얼마나 유사하거나 다른지 쉽게 확인할 수 있습니다. 의미 사이의 \"거리\"를 측정하는 방법으로 생각하십시오. 이를 통해 다음을 수행할 수 있습니다.\n",
        "\n",
        "\n",
        "   - 관련 주제의 문서 찾기\n",
        "   - 시각적으로 유사한 이미지를 찾아보세요\n",
        "   - 유사한 내용의 영상을 그룹화\n",
        "\n",
        "\n",
        "* **검색 및 추천:** 특정 이미지를 찾고 있는데 해당 이미지를 검색할 적절한 키워드가 없다고 상상해 보세요. 임베딩을 사용하면 예시 이미지만 제공하여 검색 엔진에서 시각적 스타일이나 콘텐츠가 유사한 다른 이미지를 찾을 수 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **기계 학습 및 AI:** 임베딩은 많은 최신 AI 애플리케이션의 중추입니다. 텍스트를 이해 및 생성하고, 이미지를 분류하고, 언어를 번역하고, 심지어 예술 작품을 만들 수도 있는 모델을 훈련하는데 사용됩니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCFSGAolERQI"
      },
      "source": [
        "텍스트, 이미지 또는 모든 콘텐츠와 같은 특정 콘텐츠로 훈련되면 AI는 본질적으로 콘텐츠의 의미에 대한 맵인 \"임베딩 공간\"이라는 공간을 생성합니다.\n",
        "\n",
        "\n",
        "![](https://storage.googleapis.com/github-repo/img/embeddings/textemb-vs-notebook/3.png)\n",
        "\n",
        "\n",
        "AI는 지도에서 각 콘텐츠의 위치를 ​​식별할 수 있습니다. 이것이 바로 임베딩입니다.\n",
        "\n",
        "\n",
        "![](https://storage.googleapis.com/github-repo/img/embeddings/textemb-vs-notebook/4.png)\n",
        "\n",
        "\n",
        "영화, 음악, 배우에 대해 각각 10%, 2%, 30%의 분포로 설명하는 텍스트를 예로 들어 보겠습니다. 이 경우 AI는 3차원 공간에서 0.1, 0.02, 0.3의 세 가지 값을 사용하여 임베딩을 생성할 수 있습니다.\n",
        "\n",
        "\n",
        "![](https://storage.googleapis.com/github-repo/img/embeddings/textemb-vs-notebook/5.png)\n",
        "\n",
        "\n",
        "AI는 비슷한 의미의 콘텐츠를 공간에 촘촘하게 배치할 수 있습니다.\n",
        "\n",
        "\n",
        "AI와 임베딩은 이제 인간-컴퓨터 상호 작용의 새로운 방식을 만드는 데 중요한 역할을 하고 있습니다.\n",
        "\n",
        "\n",
        "![](https://storage.googleapis.com/github-repo/img/embeddings/textemb-vs-notebook/6.png)\n",
        "\n",
        "\n",
        "AI는 데이터를 임베딩으로 구성합니다. 이는 사용자가 찾고 있는 것, 콘텐츠의 의미 또는 비즈니스에서 가지고 있는 다른 많은 것들을 나타냅니다. 이는 새로운 표준이 되고 있는 새로운 수준의 사용자 경험을 창출합니다.\n",
        "\n",
        "\n",
        "임베딩에 대해 자세히 알아보려면 [기초 과정: Google 머신러닝에 대한 임베딩 집중 과정](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture) 및 [AI의 멀티툴을 만나보세요: Vector Dale Markowitz의 임베딩](https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-Vector-embeddings)은 훌륭한 자료입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWVXvBnAEY8w"
      },
      "source": [
        "AI와 임베딩은 이제 인간-컴퓨터 상호 작용의 새로운 방식을 만드는 데 중요한 역할을 하고 있습니다.\n",
        "\n",
        "![](https://storage.googleapis.com/github-repo/img/embeddings/textemb-vs-notebook/6.png)\n",
        "\n",
        "AI는 데이터를 임베딩으로 구성합니다. 이는 사용자가 찾고 있는 것, 콘텐츠의 의미 또는 비즈니스에서 가지고 있는 다른 많은 것들을 나타냅니다. 이는 새로운 표준이 되고 있는 새로운 수준의 사용자 경험을 창출합니다.\n",
        "\n",
        "임베딩에 대해 자세히 알아보려면 [기초 과정: Google 머신러닝에 대한 임베딩 집중 과정](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture) 및 [AI의 멀티툴을 만나보세요: Vector Dale Markowitz의 임베딩](https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-Vector-embeddings)은 훌륭한 자료입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQT500QqVPIb"
      },
      "source": [
        "### 목적\n",
        "\n",
        "\n",
        "이 노트북에서는 다음을 탐색합니다.\n",
        "* Vertex AI 텍스트 임베딩 API\n",
        "* Vertex AI 멀티모달 임베딩 API(이미지 및 비디오)\n",
        "* 전자상거래 데이터로 간편검색 구축\n",
        "   - 텍스트 쿼리를 기반으로 제품 찾기\n",
        "   - 이미지를 기반으로 상품 찾기\n",
        "   - 영상을 기반으로 영상 찾기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnpYxfesh2rI"
      },
      "source": [
        "### 비용\n",
        "\n",
        "\n",
        "이 튜토리얼에서는 비용이 청구되는 Google Cloud 구성요소를 사용합니다.\n",
        "\n",
        "\n",
        "- Vertex AI\n",
        "\n",
        "\n",
        "[Vertex AI 가격](https://cloud.google.com/vertex-ai/pricing)에 대해 알아보고 [가격 계산기](https://cloud.google.com/products/calculator/)를 사용하여 예상 사용량을 기준으로 한 비용 견적입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXJpXzKrh2rJ"
      },
      "source": [
        "## 시작하기\n",
        "### Python 및 기타 dependencies를 위해 Vertex AI SDK 설치하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc4WxYmLSBW5",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeaf6faf-633a-4ba2-e94c-c39921a0dd68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip3 install -q --upgrade --user google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### 현재 런타임 다시 시작\n",
        "\n",
        "\n",
        "이 Jupyter 런타임에서 새로 설치된 패키지를 사용하려면 런타임을 다시 시작해야 합니다. 아래 셀을 실행하면 현재 커널이 다시 시작됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c388c86-e1b0-4f23-813a-dbf7bb1e3d12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "import time\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ 커널이 재시작될 것입니다. 다음 단계로 넘어가기 전에 재시작이 완료될 때까지 잠시만 기다려주시기 바랍니다. ⚠️</b>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtsU9Bw9h2rL"
      },
      "source": [
        "### 노트북 환경 인증(Colab만 해당)\n",
        "\n",
        "Google Colab에서 이 노트북을 실행하는 경우 다음 셀을 실행하여 환경을 인증하세요. [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)를 사용하는 경우에는 이 단계가 필요하지 않습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpYEyLsOh2rL",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Additional authentication is required for Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1vKZZoEh2rL"
      },
      "source": [
        "### Google Cloud 프로젝트 정보 설정 및 Vertex AI SDK 초기화\n",
        "\n",
        "Vertex AI 사용을 시작하려면 기존 Google Cloud 프로젝트가 있어야 하며 [Vertex AI API를 사용 설정](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)해야 합니다.\n",
        "\n",
        "[프로젝트 및 개발 환경 설정](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)에 대해 자세히 알아보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJqZ76rJh2rM",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27c10d75-825e-4307-8cb7-dd52fc3e8e39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your project ID is: thuya-genai\n"
          ]
        }
      ],
      "source": [
        "# Define project information\n",
        "\n",
        "import sys\n",
        "\n",
        "PROJECT_ID = \"thuya-genai\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# if not running on colab, try to get the PROJECT_ID automatically\n",
        "if \"google.colab\" not in sys.modules:\n",
        "    import subprocess\n",
        "\n",
        "    PROJECT_ID = subprocess.check_output(\n",
        "        [\"gcloud\", \"config\", \"get-value\", \"project\"], text=True\n",
        "    ).strip()\n",
        "\n",
        "print(f\"Your project ID is: {PROJECT_ID}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdA6aELP0aH0"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Initialize Vertex AI\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuQwwRiniVFG"
      },
      "source": [
        "### 라이브러리 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtMowvm-yQ97",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from base64 import b64encode\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import requests\n",
        "from PIL import Image as PILImage  # Explicit import for clarity\n",
        "from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
        "import vertexai\n",
        "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
        "from vertexai.vision_models import (\n",
        "    Image as VMImage,\n",
        "    MultiModalEmbeddingModel,\n",
        "    MultiModalEmbeddingResponse,\n",
        "    Video as VMVideo,\n",
        "    VideoSegmentConfig,\n",
        ")\n",
        "\n",
        "pd.options.mode.chained_assignment = None  # default='warn'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-TX_R_xh2rM"
      },
      "source": [
        "### Vertex AI 텍스트 및 멀티모달 임베딩 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvMwSRJJh2rM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "mm_embedding_model = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding\")\n",
        "text_embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLfNXQLMyj43"
      },
      "source": [
        "## Vertex AI 임베딩 API의 기본 사항\n",
        "\n",
        "\n",
        "## 텍스트 임베딩\n",
        "\n",
        "\n",
        "텍스트 임베딩은 콘텐츠의 조밀하고 저차원적인 벡터 표현으로, 두 콘텐츠가 의미상 유사할 경우 해당 임베딩이 임베딩 벡터 공간에서 서로 가까이 위치하게 됩니다. 이 표현은 다음과 같은 일반적인 NLP 작업을 해결하는 데 사용될 수 있습니다.\n",
        "\n",
        "\n",
        "* **의미 검색**: 의미 유사성에 따라 순위가 매겨진 검색 텍스트입니다.\n",
        "* **권장사항**: 주어진 텍스트와 유사한 텍스트 속성을 가진 항목을 반환합니다.\n",
        "* **분류**: 텍스트 속성이 주어진 텍스트와 유사한 항목의 클래스를 반환합니다.\n",
        "* **클러스터링**: 텍스트 속성이 지정된 텍스트와 유사한 클러스터 항목입니다.\n",
        "* **이상치 감지**: 텍스트 속성이 지정된 텍스트와 가장 관련이 없는 항목을 반환합니다.\n",
        "\n",
        "\n",
        "자세한 내용은 [텍스트 임베딩 모델 문서](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings)를 참조하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v28L_YJRzcEK",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def embed_text(\n",
        "    texts: List[str] = [\"banana muffins? \", \"banana bread? banana muffins?\"],\n",
        "    model: TextEmbeddingModel = text_embedding_model,\n",
        "    task: str = \"RETRIEVAL_DOCUMENT\",\n",
        "    dimensionality: Optional[int] = 768,\n",
        ") -> List[List[float]]:\n",
        "\n",
        "    \"\"\"Embeds texts with a pre-trained, foundational model.\"\"\"\n",
        "    inputs = [TextEmbeddingInput(text, task) for text in texts]\n",
        "    kwargs = dict(output_dimensionality=dimensionality) if dimensionality else {}\n",
        "    embeddings = model.get_embeddings(inputs, **kwargs)\n",
        "    return [embedding.values for embedding in embeddings]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beaMkDH9zlr2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "tex_embedding = embed_text(texts=[\"What is life?\"])\n",
        "print(\"length of embedding: \", len(tex_embedding[0]))\n",
        "print(\"First five vectors are: \", tex_embedding[0][:5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tex_embedding2 = embed_text(texts=[\"What is love?\"])\n",
        "print(\"length of embedding: \", len(tex_embedding2[0]))\n",
        "print(\"First five vectors are: \", tex_embedding2[0][:5])"
      ],
      "metadata": {
        "id": "orprORlQ883I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oug-MkjFywzc"
      },
      "source": [
        "#### 임베딩 및 Pandas DataFrame\n",
        "\n",
        "\n",
        "텍스트가 DataFrame의 열에 저장된 경우 아래 예를 사용하여 임베딩이 포함된 새 열을 만들 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trpS0J6ByNd0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "text = [\n",
        "    \"i really enjoyed the movie last night\",\n",
        "    \"so many amazing cinematic scenes yesterday\",\n",
        "    \"had a great time writing my Python scripts a few days ago\",\n",
        "    \"huge sense of relief when my .py script finally ran without error\",\n",
        "    \"O Romeo, Romeo, wherefore art thou Romeo?\",\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(text, columns=[\"text\"])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luwdn0qA0QZN"
      },
      "source": [
        "임베딩 모델이 포함된 pandas의 [apply](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html) 함수를 사용하여 'embeddings'라는 새 열을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etrempeSyQ9V"
      },
      "outputs": [],
      "source": [
        "df[\"embeddings\"] = df.apply(lambda x: embed_text([x.text])[0], axis=1)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXMu9UYs0Z8S"
      },
      "source": [
        "#### 코사인 유사성을 사용하여 텍스트 예제의 유사성 비교\n",
        "\n",
        "\n",
        "텍스트를 임베딩으로 변환하여 유사성 점수를 계산할 수 있습니다. 유사성 점수를 계산하는 방법에는 여러 가지가 있으며, 일반적인 기술 중 하나는 [코사인 유사성](https://en.wikipedia.org/wiki/Cosine_similarity)을 사용하는 것입니다.\n",
        "\n",
        "\n",
        "위의 예에서 `text` 열의 문장 중 두 개는 _영화_ 즐기기에 관련되고, 나머지 두 개는 _코딩_ 즐기기에 관련됩니다. 의미상으로 관련된 문장 간의 쌍 비교를 수행할 때는 코사인 유사성 점수가 더 높아야 하며(1.0에 가까움), 의미가 다른 문장 사이에서는 점수가 낮아야 합니다.\n",
        "\n",
        "\n",
        "아래 DataFrame 출력은 임베딩 간의 결과 코사인 유사성 점수를 보여줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXtUBuV9y4oX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cos_sim_array = cosine_similarity(list(df.embeddings.values))\n",
        "\n",
        "# display as DataFrame\n",
        "df = pd.DataFrame(cos_sim_array, index=text, columns=text)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFE6uErz0gGe"
      },
      "source": [
        "\n",
        "이를 더 쉽게 이해하기 위해 히트맵을 사용할 수 있습니다. 당연히 텍스트는 동일할 때(점수 1.0) 가장 유사합니다. 다음으로 높은 점수는 문장이 의미적으로 유사한 경우입니다. 가장 낮은 점수는 문장의 의미가 상당히 다를 때입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E4cfugj0gmC"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "ax = sns.heatmap(df, annot=True, cmap=\"crest\")\n",
        "ax.xaxis.tick_top()\n",
        "ax.set_xticklabels(text, rotation=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF0NNyDv13fX"
      },
      "source": [
        "## 멀티모달 임베딩 API\n",
        "\n",
        "[멀티모달 임베딩](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings) 모델은 [ 128, 256, 512 및 1408(기본값) ]을 생성합니다. 이미지, 텍스트 및 비디오 데이터의 조합을 포함할 수 있는 입력을 기반으로 하는 차원 벡터입니다. 그런 다음 임베딩 벡터를 이미지 분류 또는 비디오 콘텐츠 조정과 같은 후속 작업에 사용할 수 있습니다.\n",
        "\n",
        "이미지 임베딩 벡터와 텍스트 임베딩 벡터는 동일한 차원을 갖는 동일한 의미 공간에 있습니다. 결과적으로 이러한 벡터는 텍스트로 이미지를 검색하거나 이미지로 비디오를 검색하는 등의 사용 사례에 대해 상호 교환적으로 사용될 수 있습니다.\n",
        "\n",
        "텍스트 전용 임베딩 사용 사례의 경우 위에서 설명한 대신 Vertex AI 텍스트 임베딩 API를 사용하는 것이 좋습니다.\n",
        "\n",
        "**사용 사례**\n",
        "\n",
        "**이미지 및 텍스트:**\n",
        "\n",
        "\n",
        "* 이미지 분류: 이미지를 입력으로 가져와 하나 이상의 클래스(레이블)를 예측합니다.\n",
        "* 이미지 검색: 관련성이 있거나 유사한 이미지를 검색합니다.\n",
        "* 추천: 이미지를 기반으로 제품 또는 광고 추천을 생성합니다.\n",
        "\n",
        "\n",
        "\n",
        "**이미지, 텍스트, 동영상:**\n",
        "\n",
        "* 추천 : 영상을 기반으로 상품이나 광고 추천을 생성합니다(유사성 검색).\n",
        "* 비디오 콘텐츠 검색\n",
        "    * 의미 검색 사용: 텍스트를 입력으로 사용하고 쿼리와 일치하는 순위 프레임 집합을 반환합니다.\n",
        "* 유사성 검색 사용:\n",
        "    * 비디오를 입력으로 사용하고 쿼리와 일치하는 비디오 세트를 반환합니다.\n",
        "    * 이미지를 입력으로 사용하고 쿼리와 일치하는 동영상 세트를 반환합니다.\n",
        "* 비디오 분류: 비디오를 입력으로 사용하고 하나 이상의 클래스를 예측합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olQ6tojAQR5V"
      },
      "outputs": [],
      "source": [
        "def get_image_video_text_embeddings(\n",
        "    image_path: Optional[str] = None,\n",
        "    video_path: Optional[str] = None,\n",
        "    contextual_text: Optional[str] = None,\n",
        "    dimension: Optional[int] = None,\n",
        "    video_segment_config: Optional[VideoSegmentConfig] = None,\n",
        "    debug: bool = False,\n",
        ") -> MultiModalEmbeddingResponse:\n",
        "    \"\"\"Generates multimodal embeddings from image, video, and text.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to image (local or Google Cloud Storage).\n",
        "        video_path: Path to video (local or Google Cloud Storage).\n",
        "        contextual_text: Text to generate embeddings for. Max: 32 tokens (~32 words).\n",
        "        dimension: Dimension of the returned embeddings (128, 256, 512, or 1408).\n",
        "        video_segment_config: Defines specific video segments for embedding generation.\n",
        "        debug: If True, print debugging information.\n",
        "\n",
        "    Returns:\n",
        "        MultiModalEmbeddingResponse: The generated embeddings.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If neither image_path, video_path, nor contextual_text is provided.\n",
        "    \"\"\"\n",
        "\n",
        "    # Input validation\n",
        "    if not any([image_path, video_path, contextual_text]):\n",
        "        raise ValueError(\n",
        "            \"At least one of image_path, video_path, or contextual_text must be provided.\"\n",
        "        )\n",
        "\n",
        "    image = VMImage.load_from_file(image_path) if image_path else None\n",
        "    video = VMVideo.load_from_file(video_path) if video_path else None\n",
        "\n",
        "    embeddings = mm_embedding_model.get_embeddings(\n",
        "        image=image,\n",
        "        video=video,\n",
        "        video_segment_config=video_segment_config,\n",
        "        contextual_text=contextual_text,\n",
        "        dimension=dimension,\n",
        "    )\n",
        "\n",
        "    # Prepare result dictionary for better organization\n",
        "    result = {}\n",
        "\n",
        "    if image_path:\n",
        "        if debug:\n",
        "            print(\n",
        "                f\"Image Embedding (first five):\\n{embeddings.image_embedding[:5]}\"\n",
        "            )\n",
        "            print(f\"Dimension of Image Embedding: {len(embeddings.image_embedding)}\")\n",
        "        result[\"image_embedding\"] = embeddings.image_embedding\n",
        "\n",
        "    if video_path:\n",
        "        if debug:\n",
        "            print(\"Video Embeddings:\")\n",
        "        video_embedding_list = [\n",
        "            {\n",
        "                \"start_offset_sec\": video_embedding.start_offset_sec,\n",
        "                \"end_offset_sec\": video_embedding.end_offset_sec,\n",
        "                \"embedding\": video_embedding.embedding,\n",
        "            }\n",
        "            for video_embedding in embeddings.video_embeddings\n",
        "        ]\n",
        "        result[\"video_embeddings\"] = video_embedding_list\n",
        "\n",
        "        if debug:\n",
        "            for embedding in video_embedding_list:\n",
        "                print(\n",
        "                    f\"\\nVideo Segment (in seconds): {embedding['start_offset_sec']} - {embedding['end_offset_sec']}\"\n",
        "                )\n",
        "                print(f\"Embedding (first five): {embedding['embedding'][:5]}\")\n",
        "                print(f\"Dimension of Video Embedding: {len(embedding['embedding'])}\")\n",
        "\n",
        "    if contextual_text:\n",
        "        if debug:\n",
        "            print(f\"\\n\\nText Embedding (first five):\\n{embeddings.text_embedding[:5]}\")\n",
        "            print(f\"Dimension of Text Embedding: {len(embeddings.text_embedding)}\")\n",
        "        result[\"text_embedding\"] = embeddings.text_embedding\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서는 이 이미지에 대한 임베딩을 생성해 보겠습니다.\n",
        "\n",
        "<a href=\"https://storage.googleapis.com/github-repo/embeddings/getting_started_embeddings/gms_images/GGOEACBA104999.jpg\"><img src=\"https://storage.googleapis.com/github-repo/embeddings/getting_started_embeddings/gms_images/GGOEACBA104999.jpg\" width=\"200px\"/></a>\n"
      ],
      "metadata": {
        "id": "6cgS9TsmzzPg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10kzx4YoVjMU"
      },
      "outputs": [],
      "source": [
        "# Image embeddings with default 1408 dimension\n",
        "result = get_image_video_text_embeddings(\n",
        "    image_path=\"gs://github-repo/embeddings/getting_started_embeddings/gms_images/GGOEACBA104999.jpg\",\n",
        "    debug=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRYoTJqwwu9D"
      },
      "outputs": [],
      "source": [
        "# Image embeddings with 256 dimension\n",
        "result = get_image_video_text_embeddings(\n",
        "    image_path=\"gs://github-repo/embeddings/getting_started_embeddings/gms_images/GGOEAFKA194799.jpg\",\n",
        "    dimension=256,  # Available values: 128, 256, 512, and 1408 (default)\n",
        "    debug=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 이 비디오에 대한 임베딩을 생성하겠습니다."
      ],
      "metadata": {
        "id": "ERG-WJuM2x-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Video\n",
        "\n",
        "Video(\"https://storage.googleapis.com/github-repo/embeddings/getting_started_embeddings/UCF-101-subset/BrushingTeeth/v_BrushingTeeth_g01_c02.mp4\")\n"
      ],
      "metadata": {
        "id": "1aZ-M_z72mDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeGOaYxGwtdp"
      },
      "outputs": [],
      "source": [
        "# Video embedding only supports 1408 dimension\n",
        "result = get_image_video_text_embeddings(\n",
        "    video_path=\"gs://github-repo/embeddings/getting_started_embeddings/UCF-101-subset/BrushingTeeth/v_BrushingTeeth_g01_c02.mp4\",\n",
        "    debug=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOa8OrJpyYlK"
      },
      "outputs": [],
      "source": [
        "# Video embeddings with 1408 dimensions and Video Segment.\n",
        "# Video embedding only supports 1408 dimensions for now.\n",
        "\n",
        "result = get_image_video_text_embeddings(\n",
        "    video_path=\"gs://github-repo/embeddings/getting_started_embeddings/Googles newest and most capable AI  Gemini.mp4\",\n",
        "    video_segment_config=VideoSegmentConfig(\n",
        "        start_offset_sec=0, end_offset_sec=120, interval_sec=60\n",
        "    ),\n",
        "    debug=True,\n",
        ")  # printing first 10 vectors only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlOmaXeEXV9p"
      },
      "source": [
        "## 사용 사례: 임베딩을 사용하여 간단한 검색 구축\n",
        "\n",
        "\n",
        "### 처음부터 데이터에 대한 임베딩 구축\n",
        "\n",
        "\n",
        "#### 동영상 임베딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r17fSDV8Uy-p"
      },
      "outputs": [],
      "source": [
        "data_url = \"https://storage.googleapis.com/github-repo/embeddings/getting_started_embeddings/video_data_without_embeddings.csv\"\n",
        "video_data_without_embeddings = pd.read_csv(data_url)\n",
        "video_data_without_embeddings.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El2wJCTUQ5ux"
      },
      "outputs": [],
      "source": [
        "# This will take around 3 minutes\n",
        "\n",
        "# Create new dataframe to store embeddings\n",
        "video_data_without_embeddings = pd.read_csv(data_url)\n",
        "video_data_with_embeddings = video_data_without_embeddings.copy()\n",
        "\n",
        "# Get Video Embeddings\n",
        "video_data_with_embeddings[\"video_embeddings\"] = (\n",
        "    video_data_without_embeddings[\"gcs_path\"]\n",
        "    .apply(\n",
        "        lambda x: get_image_video_text_embeddings(video_path=x)['video_embeddings'][0]['embedding']\n",
        "    )\n",
        ")\n",
        "\n",
        "video_data_with_embeddings.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY3eurefXZ1E"
      },
      "source": [
        "#### 이미지 및 텍스트 임베딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzAteOhSSr8K"
      },
      "outputs": [],
      "source": [
        "data_url = \"https://storage.googleapis.com/github-repo/embeddings/getting_started_embeddings/image_data_without_embeddings.csv\"\n",
        "image_data_without_embeddings = pd.read_csv(data_url)\n",
        "image_data_without_embeddings.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LExwGZuKSEbZ"
      },
      "outputs": [],
      "source": [
        "# This will take around 5 minutes\n",
        "\n",
        "# Create new dataframe to store embeddings\n",
        "image_data_with_embeddings = image_data_without_embeddings.copy()\n",
        "\n",
        "# Combining all the text metadata in a single column.\n",
        "image_data_with_embeddings[\"combined_text\"] = (\n",
        "    image_data_with_embeddings[\"title\"].fillna(\"\")\n",
        "    + \" \"\n",
        "    + image_data_with_embeddings[\"keywords\"].fillna(\"\")\n",
        "    + \" \"\n",
        "    + image_data_with_embeddings[\"metadescription\"].fillna(\"\")\n",
        ")\n",
        "\n",
        "# Get Image and Text Embeddings\n",
        "\n",
        "# Taking default 1408 dimension\n",
        "image_data_with_embeddings[\"image_embeddings\"] = image_data_with_embeddings[\n",
        "    \"gcs_path\"\n",
        "].apply(lambda x: get_image_video_text_embeddings(image_path=x)[\"image_embedding\"])\n",
        "# Taking default 768 dimension\n",
        "image_data_with_embeddings[\"text_embeddings\"] = image_data_with_embeddings[\n",
        "    \"combined_text\"\n",
        "].apply(lambda x: embed_text([x])[0])\n",
        "\n",
        "image_data_with_embeddings.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1TyK-ENVlHf"
      },
      "source": [
        "## 미리 계산된 임베딩 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nigd4xO7Vow5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Comment this cell, if you are computing embeddings from scratch.\n",
        "\n",
        "# image_data_with_embeddings = pd.read_csv(\n",
        "#     \"https://storage.googleapis.com/github-repo/embeddings/getting_started_embeddings/image_data_with_embeddings.csv\"\n",
        "# )  # dimensions; image = 1408, text = 768\n",
        "\n",
        "# image_data_with_embeddings.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3aF8-A0V9mF",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# video_data_with_embeddings = pd.read_csv(\n",
        "#     \"https://storage.googleapis.com/github-repo/embeddings/getting_started_embeddings/video_data_with_embeddings.csv\"\n",
        "# )  # dimensions; video = 1408\n",
        "\n",
        "# video_data_with_embeddings.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzlJc6V9A_R5"
      },
      "source": [
        "## 사용 사례 - 이미지 및 텍스트\n",
        "\n",
        "### 텍스트 검색어를 기반으로 상품을 찾아보세요.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mxZkwefyAzD"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image as ImageByte, display\n",
        "\n",
        "\n",
        "def get_url_from_gcs(gcs_uri: str) -> str:\n",
        "    \"\"\"Converts a Google Cloud Storage (GCS) URI to a publicly accessible URL.\n",
        "\n",
        "    Args:\n",
        "        gcs_uri: The GCS URI in the format gs://bucket-name/object-name.\n",
        "\n",
        "    Returns:\n",
        "        The corresponding public URL for the object.\n",
        "    \"\"\"\n",
        "    return gcs_uri.replace(\"gs://\", \"https://storage.googleapis.com/\").replace(\n",
        "        \" \", \"%20\"\n",
        "    )\n",
        "\n",
        "\n",
        "def print_shortlisted_products(\n",
        "    shortlisted_products: Union[Dict, Any], display_flag: bool = False\n",
        ") -> None:\n",
        "    \"\"\"Prints information about shortlisted products, optionally displaying images.\n",
        "\n",
        "    Args:\n",
        "        shortlisted_products: A dictionary-like object containing product data with 'score' and 'gcs_path' keys.\n",
        "        display_flag: If True, displays images of the products using IPython.\n",
        "    \"\"\"\n",
        "    print(\"Similar product identified ---- \\n\")\n",
        "\n",
        "    for (\n",
        "        index,\n",
        "        product,\n",
        "    ) in shortlisted_products.iterrows():  # Iterate directly over rows for clarity\n",
        "        score = product[\"score\"]\n",
        "        gcs_path = product[\"gcs_path\"]\n",
        "        url = get_url_from_gcs(gcs_path)\n",
        "\n",
        "        print(f\"Product {index + 1}: Confidence Score: {score}\")\n",
        "        print(url)\n",
        "\n",
        "        if display_flag:\n",
        "            # display(ImageByte(url=url), width=200)  # Simplified image display\n",
        "            display(ImageByte(url, width=200))\n",
        "            print()  # Add an empty line for visual separation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEQ7SK4VIZe7"
      },
      "source": [
        "**코사인 유사성: 크기가 아닌 방향의 척도**\n",
        "\n",
        "\n",
        "코사인 유사성은 두 벡터(예: 공간의 점) 간의 유사성을 측정하는 수학적 도구입니다. 벡터를 화살표로 생각해보세요.\n",
        "* **방향:** 각 벡터는 특정 방향을 가리킵니다.\n",
        "* **길이:** 각 벡터에는 특정 길이(크기)가 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "코사인 유사성은 길이를 무시하고 두 벡터 사이의 각도에 중점을 둡니다. 두 벡터가 거의 같은 방향을 가리키면 매우 유사한 것으로 간주됩니다(코사인 유사도가 1에 가까움). 직각에 있으면 완전히 다른 것입니다(코사인 유사도 0). 반대 방향은 -1의 유사성을 제공합니다.\n",
        "\n",
        "\n",
        "[코사인 유사성](https://en.wikipedia.org/wiki/Cosine_similarity)에 대해 자세히 알아보세요.\n",
        "\n",
        "\n",
        "**임베딩: 데이터를 벡터로 변환**\n",
        "\n",
        "\n",
        "기계 학습에서 임베딩은 복잡한 데이터(예: 텍스트, 이미지, 비디오)를 벡터로 표현하는 방법입니다. 이를 통해 코사인 유사성과 같은 수학적 연산을 적용할 수 있습니다. 예를 들어:\n",
        "\n",
        "\n",
        "* **단어 임베딩:** 비슷한 의미를 가진 단어에는 비슷한 방향을 가리키는 벡터가 있습니다.\n",
        "* **이미지 임베딩:** 유사한 콘텐츠(예: 고양이 사진 두 장)가 있는 이미지에는 사이에 작은 각도가 있는 벡터가 있습니다.\n",
        "* **동영상 임베딩:** 동일한 이벤트 또는 유사한 주제의 동영상에는 유사한 벡터 표현이 있습니다.\n",
        "\n",
        "\n",
        "**벡터의 기타 거리 측정법**\n",
        "\n",
        "\n",
        "코사인 유사성은 널리 사용되지만 벡터 간의 거리를 측정하는 유일한 방법은 아닙니다. 다음은 몇 가지 다른 사항입니다.\n",
        "\n",
        "\n",
        "* **유클리드 거리:** 두 점 사이의 직선 거리입니다. 크기의 차이에 민감합니다.\n",
        "* **맨해튼 거리:** 그리드(예: 도시 블록)를 따라서만 이동할 수 있는 경우 두 지점 사이의 거리입니다.\n",
        "* **해밍 거리:** 두 벡터가 서로 다른 위치의 수입니다(종종 이진 데이터에 사용됨).\n",
        "\n",
        "\n",
        "**올바른 거리 측정법 선택**\n",
        "가장 좋은 거리 측정법은 특정 애플리케이션에 따라 다릅니다. 코사인 유사성은 항목의 절대 크기보다는 항목 간의 관계(예: 단어의 의미, 이미지 내용)에 더 관심이 있을 때 선호되는 경우가 많습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIiC_5JxWRMw"
      },
      "outputs": [],
      "source": [
        "search_query = \"I am looking for something related to dinosaurs theme\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96XUS3lDYikY"
      },
      "outputs": [],
      "source": [
        "# Steps to get similar products\n",
        "\n",
        "# Step 1 - Convert 'search_query' to embeddings\n",
        "search_query_embedding = embed_text(texts=[search_query],\n",
        "                                    dimensionality=512)[0]\n",
        "\n",
        "# Step 2 - Find cosine similarity (or simply np.dot) of search_query_embedding and image_data_with_embeddings['text_embeddings']\n",
        "cosine_scores = image_data_with_embeddings[\"text_embeddings\"].apply(\n",
        "    lambda x: round(np.dot(x, search_query_embedding), 2)\n",
        ")  # eval is used to convert string of list to list\n",
        "\n",
        "# Step 3 - Sort the cosine score and pick the top 3 results\n",
        "top_3_indices = cosine_scores.nlargest(3).index.tolist()\n",
        "top_n_cosine_values = cosine_scores.nlargest(3).values.tolist()\n",
        "\n",
        "# Step 4 - Filter image_data_with_embeddings with the shortlisted index and add score\n",
        "shortlisted_products = image_data_with_embeddings.iloc[top_3_indices]\n",
        "shortlisted_products.loc[:, \"score\"] = top_n_cosine_values\n",
        "\n",
        "# Step 5 - Display the shortlisted products\n",
        "print_shortlisted_products(\n",
        "    shortlisted_products,\n",
        "    display_flag=True\n",
        ")  # pass display_flag=True to display the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwf5m588zs33"
      },
      "outputs": [],
      "source": [
        "def get_similar_products_from_text_query(\n",
        "    search_query: str,\n",
        "    top_n: int = 3,\n",
        "    threshold: float = 0.6,\n",
        "    embeddings_data: pd.DataFrame = image_data_with_embeddings,\n",
        ") -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Retrieves the most similar products to a given text query based on embeddings.\n",
        "\n",
        "    Args:\n",
        "        search_query: The text query to find similar products for.\n",
        "        top_n: The maximum number of similar products to return.\n",
        "        threshold: The minimum cosine similarity score for a product to be considered similar.\n",
        "        embeddings_data: A DataFrame containing text embeddings for products (assumes a column named 'text_embeddings').\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame with the top_n similar products and their scores, or None if none meet the threshold.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Embed the search query\n",
        "    search_query_embedding = embed_text([search_query])[\n",
        "        0\n",
        "    ]  # Assuming 'embed_text' is a function you have\n",
        "\n",
        "    # Step 2: Calculate cosine similarities (optimize by avoiding apply and eval)\n",
        "    cosine_scores = image_data_with_embeddings[\"text_embeddings\"].apply(\n",
        "        lambda x: round(np.dot(eval(x), search_query_embedding), 2)\n",
        "    )\n",
        "\n",
        "    # Step 3: Filter and sort scores\n",
        "    scores_above_threshold = cosine_scores[cosine_scores >= threshold]\n",
        "    top_n_indices = scores_above_threshold.nlargest(top_n).index.tolist()\n",
        "\n",
        "    # Step 4: Handle cases with insufficient scores\n",
        "    if len(top_n_indices) < top_n:\n",
        "        print(f\"Warning: Only {len(top_n_indices)} scores meet the threshold.\")\n",
        "\n",
        "    # Step 5: Get shortlisted products with scores (optimize by direct assignment)\n",
        "    if top_n_indices:\n",
        "        shortlisted_products = embeddings_data.iloc[top_n_indices].copy()\n",
        "        shortlisted_products[\"score\"] = scores_above_threshold.nlargest(\n",
        "            top_n\n",
        "        ).values.tolist()\n",
        "    else:\n",
        "        print(\"No scores meet the threshold. Consider lowering the threshold.\")\n",
        "        shortlisted_products = None\n",
        "\n",
        "    return shortlisted_products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3Dui6Y1gM4G"
      },
      "outputs": [],
      "source": [
        "search_query = \"Do you have socks in checkered patterns?\"\n",
        "\n",
        "shortlisted_products = get_similar_products_from_text_query(\n",
        "    search_query, top_n=3, threshold=0.7\n",
        ")\n",
        "\n",
        "print_shortlisted_products(shortlisted_products)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSdmrJ_gqUtQ"
      },
      "outputs": [],
      "source": [
        "search_query = \"I am looking for sweatshirt with google logo and probably should be in embroidered pattern\"\n",
        "\n",
        "shortlisted_products = get_similar_products_from_text_query(\n",
        "    search_query, top_n=3, threshold=0.6\n",
        ")\n",
        "\n",
        "print_shortlisted_products(shortlisted_products)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq8ZpN6BlrJC"
      },
      "source": [
        "### 이미지 기반 유사상품 찾기(선택상품)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuQ9lehmkuly"
      },
      "outputs": [],
      "source": [
        "random_index = 130\n",
        "liked_product = image_data_with_embeddings[\"gcs_path\"][random_index]\n",
        "print(\"Liked Product ---\")\n",
        "\n",
        "\n",
        "print(get_url_from_gcs(liked_product))\n",
        "\n",
        "# to display the image\n",
        "display(ImageByte(url=get_url_from_gcs(liked_product), width=200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnWETYpRlxSO"
      },
      "outputs": [],
      "source": [
        "# Steps to get similar products\n",
        "\n",
        "# Step 1 - Convert 'liked_product' to embeddings\n",
        "liked_product_embedding = get_image_video_text_embeddings(image_path=liked_product)[\n",
        "    \"image_embedding\"\n",
        "]\n",
        "\n",
        "# Step 2 - Find cosine similarity (or simply np.dot) of liked_product_embedding and image_data_with_embeddings['image_embeddings']\n",
        "cosine_scores = image_data_with_embeddings[\"image_embeddings\"].apply(\n",
        "    lambda x: round(np.dot(eval(x), liked_product_embedding), 2)\n",
        ")  # eval is used to convert string of list to list\n",
        "\n",
        "\n",
        "# Step 3 - Sort the cosine score, filter with threshold (matching should be less than 1.0 and greater than high value) and pick the top 2 results\n",
        "threshold = 0.6\n",
        "scores_above_threshold = cosine_scores[\n",
        "    (cosine_scores >= threshold) & (cosine_scores < 1.00)\n",
        "]\n",
        "top_2_indices = scores_above_threshold.nlargest(2).index.tolist()\n",
        "top_2_cosine_values = scores_above_threshold.nlargest(2).values.tolist()\n",
        "\n",
        "# Step 4 - Filter image_data_with_embeddings with the shortlisted index\n",
        "shortlisted_products = image_data_with_embeddings.iloc[top_2_indices]\n",
        "shortlisted_products.loc[:, \"score\"] = top_2_cosine_values\n",
        "\n",
        "# Step 5 - Display the shortlisted product.\n",
        "print_shortlisted_products(shortlisted_products, display_flag=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gUufHrb0glL"
      },
      "outputs": [],
      "source": [
        "def get_similar_products_from_image_query(\n",
        "    liked_product: Union[str, np.ndarray], top_n: int = 3, threshold: float = 0.6\n",
        ") -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Retrieves similar products based on an image query.\n",
        "\n",
        "    This function takes an image path or embedding of a \"liked\" product, compares it to\n",
        "    a dataset of product embeddings, and returns the top N most similar products\n",
        "    that exceed a specified similarity threshold.\n",
        "\n",
        "    Args:\n",
        "        liked_product: Path to the image file of the liked product or its embedding.\n",
        "        top_n: The maximum number of similar products to return.\n",
        "        threshold: The minimum cosine similarity score for a product to be considered similar.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame containing the top N similar products and their scores,\n",
        "        or None if no products meet the threshold.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Ensure the `liked_product` is an embedding\n",
        "    if isinstance(liked_product, str):\n",
        "        liked_product_embedding = get_image_video_text_embeddings(\n",
        "            image_path=liked_product\n",
        "        )[\"image_embedding\"]\n",
        "    else:\n",
        "        liked_product_embedding = liked_product\n",
        "\n",
        "    # Step 2: Calculate cosine similarities\n",
        "    # Convert embeddings to numpy arrays for efficient calculation if necessary\n",
        "    if isinstance(image_data_with_embeddings[\"image_embeddings\"].iloc[0], str):\n",
        "        image_data_with_embeddings[\"image_embeddings\"] = image_data_with_embeddings[\n",
        "            \"image_embeddings\"\n",
        "        ].apply(eval)\n",
        "    cosine_scores = image_data_with_embeddings[\"image_embeddings\"].apply(\n",
        "        lambda x: np.dot(x, liked_product_embedding)\n",
        "    )\n",
        "\n",
        "    # Step 3: Filter and select top scores\n",
        "    scores_above_threshold = cosine_scores[\n",
        "        (cosine_scores >= threshold) & (cosine_scores < 1.0)\n",
        "    ]\n",
        "    top_n_indices = scores_above_threshold.nlargest(top_n).index.tolist()\n",
        "    top_n_cosine_values = scores_above_threshold.nlargest(top_n).values.tolist()\n",
        "\n",
        "    # Step 4: Log insufficient scores (optional)\n",
        "    if len(top_n_indices) < top_n:\n",
        "        print(f\"Warning: Only {len(top_n_indices)} scores meet the threshold.\")\n",
        "\n",
        "    # Step 5: Return results if scores are available\n",
        "    if top_n_indices:\n",
        "        shortlisted_products = image_data_with_embeddings.iloc[top_n_indices].copy()\n",
        "        shortlisted_products[\"score\"] = top_n_cosine_values\n",
        "        return shortlisted_products\n",
        "\n",
        "    else:\n",
        "        print(\"No scores meet the threshold. Consider lowering the threshold.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEilR5BObZKb"
      },
      "outputs": [],
      "source": [
        "random_index = 9\n",
        "liked_product = image_data_with_embeddings[\"gcs_path\"][random_index]\n",
        "print(\"Liked Product ---\")\n",
        "print(get_url_from_gcs(liked_product))\n",
        "\n",
        "# to display the image\n",
        "# display(ImageByte(url=get_url_from_gcs(liked_product)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHQGSZh5pTaz"
      },
      "outputs": [],
      "source": [
        "# get recommendation for the liked product (image)\n",
        "shortlisted_products = get_similar_products_from_image_query(\n",
        "    liked_product, top_n=3, threshold=0.6\n",
        ")\n",
        "\n",
        "print_shortlisted_products(shortlisted_products)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHX2qX9vcE-O"
      },
      "outputs": [],
      "source": [
        "random_index = 120\n",
        "liked_product = image_data_with_embeddings[\"gcs_path\"][random_index]\n",
        "print(\"Liked Product ---\")\n",
        "\n",
        "print(get_url_from_gcs(liked_product))\n",
        "\n",
        "# to display the image\n",
        "display(ImageByte(url=get_url_from_gcs(liked_product), width=200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCVJ_vbkcIGW"
      },
      "outputs": [],
      "source": [
        "# get recommendation for the liked product (image)\n",
        "shortlisted_products = get_similar_products_from_image_query(\n",
        "    liked_product, top_n=3, threshold=0.6\n",
        ")\n",
        "\n",
        "print_shortlisted_products(shortlisted_products, display_flag=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btCIJCh2OnSP"
      },
      "source": [
        "### 비슷한 동영상 찾기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EpPyLjh1Kx0"
      },
      "outputs": [],
      "source": [
        "def display_video_gcs(\n",
        "    gcs_uri: Optional[str] = None, public_gcs_url: Optional[str] = None\n",
        ") -> None:\n",
        "    \"\"\"Displays a video hosted on Google Cloud Storage (GCS) in a Jupyter Notebook.\n",
        "\n",
        "    Args:\n",
        "        gcs_uri: The GCS URI of the video.\n",
        "        public_gcs_url: The public URL of the video, if available. If not provided and\n",
        "            `gcs_uri` is given, the function will attempt to generate the public URL.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If neither `gcs_uri` nor `public_gcs_url` is provided.\n",
        "    \"\"\"\n",
        "    if not gcs_uri and not public_gcs_url:\n",
        "        raise ValueError(\"Either gcs_uri or public_gcs_url must be provided.\")\n",
        "\n",
        "    if gcs_uri and not public_gcs_url:\n",
        "        public_gcs_url = get_url_from_gcs(\n",
        "            gcs_uri\n",
        "        )  # Assuming you have a helper function for this\n",
        "\n",
        "    html_code = f\"\"\"\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "        <source src=\"{public_gcs_url}\" type=\"video/mp4\">\n",
        "        Your browser does not support the video tag.\n",
        "    </video>\n",
        "    \"\"\"\n",
        "    display(HTML(html_code))\n",
        "\n",
        "\n",
        "def print_shortlisted_video(\n",
        "    shortlisted_videos: List[Dict], display_flag: bool = False\n",
        ") -> None:\n",
        "    \"\"\"Prints information about shortlisted videos and optionally displays them.\n",
        "\n",
        "    Args:\n",
        "        shortlisted_videos: A list of dictionaries where each dictionary represents a video\n",
        "            with keys 'score' (float) and 'gcs_path' (str).\n",
        "        display_flag: If True, displays each video in the notebook.\n",
        "    \"\"\"\n",
        "    print(\"Similar videos identified ---- \\n\")\n",
        "\n",
        "    for i in range(len(shortlisted_videos)):\n",
        "        print(f\"Video {i+1}: Confidence Score: {shortlisted_products['score'].iloc[i]}\")\n",
        "\n",
        "        url = get_url_from_gcs(shortlisted_videos[\"gcs_path\"].values[i])\n",
        "        print(url)\n",
        "        if display_flag:\n",
        "            display_video_gcs(public_gcs_url=url)\n",
        "            # IPython.display.display(load_image_from_url(url))\n",
        "            print()  # Add an empty line for visual separation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqG-qUNAeEvV"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "random_index = 10\n",
        "liked_video = video_data_with_embeddings[\"gcs_path\"][random_index]\n",
        "public_gcs_url = get_url_from_gcs(video_data_with_embeddings[\"gcs_path\"][1])\n",
        "\n",
        "print(public_gcs_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utdlMOESZ0MY"
      },
      "outputs": [],
      "source": [
        "# Steps to get similar video\n",
        "\n",
        "# Step 1 - Convert 'liked_video' to embeddings\n",
        "liked_video_embedding = get_image_video_text_embeddings(video_path=liked_video)[\n",
        "    \"video_embeddings\"\n",
        "][0][\"embedding\"]\n",
        "\n",
        "\n",
        "# Step 2 - Find cosine similarity (or simply np.dot) of liked_video_embedding and video_data_with_embeddings['video_embeddings']\n",
        "cosine_scores = video_data_with_embeddings[\"video_embeddings\"].apply(\n",
        "    lambda x: round(np.dot(eval(x), liked_video_embedding), 2)\n",
        ")  # eval is used to convert string of list to list\n",
        "\n",
        "\n",
        "# Step 3 - Sort the cosine score, filter with threshold (matching should be less than 1.0 and greater than high value) and pick the top 2 results\n",
        "threshold = 0.6\n",
        "scores_above_threshold = cosine_scores[\n",
        "    (cosine_scores >= threshold) & (cosine_scores < 1.00)\n",
        "]\n",
        "top_2_indices = scores_above_threshold.nlargest(2).index.tolist()\n",
        "top_2_cosine_values = scores_above_threshold.nlargest(2).values.tolist()\n",
        "\n",
        "# Step 4 - Filter video_data_with_embeddings with the shortlisted index\n",
        "shortlisted_videos = video_data_with_embeddings.iloc[top_2_indices]\n",
        "shortlisted_videos.loc[:, \"score\"] = top_2_cosine_values\n",
        "\n",
        "# Step 5 - Display the shortlisted video.\n",
        "print_shortlisted_video(shortlisted_videos)  # display_flag=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5Dzahkj3ar-"
      },
      "outputs": [],
      "source": [
        "def get_similar_videos_from_video_query(\n",
        "    liked_video: str, top_n: int = 3, threshold: float = 0.6\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Retrieves similar videos to a given 'liked_video' using embeddings and cosine similarity.\n",
        "\n",
        "    Args:\n",
        "        liked_video: The path to the video to find similar videos for.\n",
        "        top_n: The maximum number of similar videos to return.\n",
        "        threshold: The minimum cosine similarity score for a video to be considered similar.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame containing the shortlisted videos with their similarity scores.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Get the embeddings for the 'liked_video'\n",
        "    liked_video_embeddings = get_image_video_text_embeddings(video_path=liked_video)[\n",
        "        \"video_embeddings\"\n",
        "    ][0][\"embedding\"]\n",
        "\n",
        "    # Ensure video_data_with_embeddings is available (you might need to load or define it)\n",
        "    if \"video_data_with_embeddings\" not in globals():\n",
        "        raise ValueError(\"video_data_with_embeddings DataFrame is not defined.\")\n",
        "\n",
        "    # Step 2: Calculate cosine similarities with pre-loaded video embeddings\n",
        "    cosine_scores = video_data_with_embeddings[\"video_embeddings\"].apply(\n",
        "        lambda x: round(\n",
        "            np.dot(eval(x), liked_video_embeddings), 2\n",
        "        )  # Safely convert embeddings to arrays\n",
        "    )\n",
        "\n",
        "    # Step 3: Filter and sort based on threshold and top_n\n",
        "    scores_above_threshold = cosine_scores[\n",
        "        (cosine_scores >= threshold) & (cosine_scores < 1.00)\n",
        "    ]\n",
        "    top_indices = scores_above_threshold.nlargest(top_n).index.tolist()\n",
        "    top_cosine_values = scores_above_threshold.nlargest(top_n).values.tolist()\n",
        "\n",
        "    # Step 4: Get the shortlisted videos with scores\n",
        "    shortlisted_videos = video_data_with_embeddings.iloc[top_indices].copy()\n",
        "    shortlisted_videos[\"score\"] = top_cosine_values\n",
        "\n",
        "    return shortlisted_videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIzf9zOkmo75"
      },
      "outputs": [],
      "source": [
        "random_index = 24\n",
        "liked_video = video_data_with_embeddings[\"gcs_path\"][random_index]\n",
        "public_gcs_url = get_url_from_gcs(video_data_with_embeddings[\"gcs_path\"][1])\n",
        "\n",
        "print(public_gcs_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab9XUEBrmMxf"
      },
      "outputs": [],
      "source": [
        "shortlisted_videos = get_similar_videos_from_video_query(\n",
        "    liked_video, top_n=3, threshold=0.6\n",
        ")\n",
        "print_shortlisted_video(shortlisted_videos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IddiO0T-wdno"
      },
      "source": [
        "다음 시리즈:\n",
        "\n",
        "- 벡터(임베딩)를 Vertex Vector Store에 저장하는 방법 알아보기: [노트북](https://github.com/lavinigam-gcp/generative-ai/blob/main/embeddings/Vector-search-quickstart.ipynb)\n",
        "\n",
        "- 자신의 데이터로 임베딩을 조정하는 방법 알아보기: [노트북](https://github.com/lavinigam-gcp/generative-ai/blob/main/embeddings/intro_embeddings_tuning.ipynb)\n",
        "\n",
        "- 임베딩을 사용하여 텍스트 RAG 및 멀티모달 RAG를 수행하는 방법 알아보기: [노트북](https://github.com/lavinigam-gcp/generative-ai/blob/main/gemini/use-cases/retrieval-augmented- Generation/ intro_multimodal_rag.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QDE1G0FID0ea"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "python3",
      "name": "common-cpu.m116",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m116"
    },
    "kernelspec": {
      "display_name": "Python 3 (Local)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}